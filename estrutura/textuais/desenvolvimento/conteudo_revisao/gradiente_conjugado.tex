\section{Método do gradiente conjugado}
Dentre os métodos mais simples podemos citar o método do gradiente descendente
ou o próprio método de Newton, mas que possuem seus problemas, e por isso,
a necessidade de métodos um pouco mais elaborados, mas ainda assim não
tão distantes da simplicidade empregadas nos outros já citados.

O método do gradiente conjugado se enquadra nessa configuração. Embora um
pouco difícil de construir o método, o seu uso não é tão mais difícil que o
do gradiente descendente.

Deve-se inciar compreendendo o que é e os problemas do gradiente descendente
afim para que se possa considerar as resoluções trazidas pelo método do
gradiente conjugado.

\subsection{Método do gradiente descendente}
O método do gradiente descendente é um dos métodos iterativos mais simples
que podemos encontrar. Ele consiste em mover o ponto da iteração atual de
acordo com a direção de declive mais acentuada, ou seja, a direção do
gradiente negativo. Considerando que queremos minimizar uma função
\(f: \mathbb{R}^n \mapsto \mathbb{R}^m \), o passo de atualização de um
ponto \( x_k \in \mathbb{R}^n \) é dada por:

\begin{equation}
x_{k+1} = x_k - \alpha_k \nabla f(x_k)
\end{equation}
\( \alpha_k \) nada mais é que um escalar que controla o tamanho da atualização
do ponto atual \(x_k\) na direção de descida. Existem várias possibilidades para
\(\alpha_k\), podemos escolher um valor constante, ou fazer uma estimativa de um
bom valor, ou ainda minimizar também esse valor afim de obter o tamanho ótimo do
passo:

\vspace{-15pt}
\begin{mini!}
{\alpha_k}{ f(x_k - \alpha_k \nabla f(x_k)) \label{gd_obj}}{\label{prob_gd}}{}
\end{mini!}

O gradiente descendente, por mais simples e prático que seja, tem seu preço.
Se pararmos para observar seu comportamento, percebemos um esforço gasto à toa.
Analisando o movimento do ponto atualizado vemos um "zig-zag", isso quer dizer que
a cada iteração, estamos desfazendo um pouco do progresso feito em uma iteração
anterior. Além disso, o gradiente negativo não aponta numa direção para o mínimo, mas
sim nos dá uma direção de minimização imediata em relação ao ponto atual. Se
considerarmos minimizar uma função quadrática convexa com seções elípticas com
alta excentricidade usando o gradiente descendente, depois de muitas iterações
teremos feito muito pouco progresso, visto que o gradiente negativo vai apenas
"rebater" o ponto entre os limites da elipse que são mais "estreitos".

Uma forma de resolver ambos os problemas, é, no lugar de considerar a direção
apenas o gradiente negativo, fazendo com que o gradiente seja ajustado a cada
iteração para ser ortogonal às direções previamente encontradas, o que impede
a perda de progresso pelo comportamento de "zig-zag". Por serem ortogonais,
eles formam um span (base) no \( \mathbb{R}^n \) possibilitando que a solução
seja escrita como uma combinação linear desse span, permitindo a cada iteração
o progresso em apenas uma direção.

\subsection{Construindo o método do gradiente conjugado}

Uma das motivações do método do gradiente conjugado \cite{bonnans2006numerical}
\cite{1987polyak} é resolver um sistema linear de \(n\) equações na forma \(Ax = b \).
Os métodos convencionais para esse tipo de problema falham no sentido de que, para um
\(n\) muito grande, é inviável ter memória para ser guardada uma segunda matriz de
mesmo tamanho, que, caso esparsa, gera um gasto de memória grande demais para pouca
informação.

Considerando \(A\) uma matriz simétrica e definida positiva, podemos definir um produto
interno com respeito à ela:

\begin{equation}
\langle u, v \rangle_A = \langle u, Av \rangle =  \langle Au, v \rangle 
\end{equation}

Onde dizemos que os vetores \(u\) e \(v\) são conjugados em respeito a \(A\) (ou A-conjugados) se:

\begin{equation}
\label{aortogonal}
\langle u, v \rangle_A = 0
\end{equation}

A partir dai podemos escolher um conjunto de \(n\) vetores que formam um span do
\( \mathbb{R}^n \):

\begin{equation}
U := \{d_1, d_2, ..., d_n\}
\end{equation}

\begin{equation}
\label{gc_prod_interno}
\langle d_i, d_j \rangle_A = 0; i \ne j
\end{equation}

Podemos escrever qualquer vetor \(v \in \mathbb{R}^n\) como uma combinação linear dos
vetores que formar \(U\):

\begin{equation}
v = \sum_{i=1}^n \alpha_i d_i
\end{equation}

Ou seja, podemos escrever a solução de \(Ax = b\) dessa forma. Chamemos a solução de
\(x^*\). Temos então:

\begin{equation}
x^* = \sum_{i=1}^n \alpha_i d_i
\end{equation}

Multiplicando A pela esquerda nos dois lados e depois multiplicando por algum \(d_k\):

\begin{equation}
Ax^* = \sum_{i=1}^n \alpha_i Ad_i \Rightarrow  d_k A x^* = \sum_{i=1}^n \alpha_i d_k A d_i
\end{equation}

Substituindo \(Ax^*\) por um vetor \(b\) e explicitando o produto interno no lado direito:

\begin{equation}
  \langle d_k, b \rangle = \sum_{i=1}^n \alpha_i d_k A d_i \Rightarrow \langle d_k, b \rangle = \sum_{i=1}^n \alpha_i \langle d_k ,d_i \rangle_A  
\end{equation}

Considerando (\ref{gc_prod_interno}), apenas a parcela \( \langle d_k, d_k \rangle_A \) não é nula, e rearranjando para
\(\alpha_k\):

\begin{equation}
\langle d_k, b \rangle  = \alpha_k \langle d_k , d_k \rangle_A \Rightarrow \frac{\langle d_k, b \rangle}{\langle d_k , d_k \rangle_A} = \alpha_k
\end{equation}

Logo, para calcular a solução diretamente, basta encontrar o conjunto \(U\).

Antes de podermos encontrar o tal conjunto, primeiro vamos observar uma outra forma de
interpretar o problema. Então, podemos escrever um problema do tipo \(Ax = b\) como um
problema de otimização de uma função quadrática convexa:

\vspace{-15pt}
\begin{mini!}
{x}{ f(x) = \frac{1}{2} \langle x, x \rangle_A - \langle b, x \rangle \label{gcq_obj}}{\label{prob_gcq}}{}
\end{mini!}

Como a função é quadrática e convexa, sabemos que possui apenas um único ponto de
extremo \(x^*\) e que é o mínimo. A partir dessas informações podemos definir as
seguintes expressões:

\begin{equation}
\nabla f(x^*) = 0
\end{equation}

Para termos \(\nabla f(x)\), podemos simplificar o processo de derivação de \(f(x)\) ao imaginar
que \(\langle x, x \rangle_A\) é de alguma forma equivalente à \(x^2\), e \(\langle x, b \rangle\)
ao produto de uma constante e a variável real. Portanto:

\begin{equation}
\nabla f(x) = Ax - b
\end{equation}

E da mesma forma:

\begin{equation*}
\nabla^2 f(x) = A
\end{equation*}

Com isso podemos usar o mesmo método para encontrar soluções para \(Ax = b\) para
minimizar funções cuja matriz hessiana (\(\nabla^2 f(x)\)), que chamaremos sempre de \(A\),
seja definida positiva.

Podemos gerar o conjunto \(U\) a partir dos gradientes, ajustando-os para que sejam
mutualmente conjugados. Escolhendo dessa forma, pode não ser necessário ter-se todos os
vetores que formam \(U\), já que como a combinação linear de alguns poucos gradientes
encontrados durante as iterações do gradiente descendente nos dão boas aproximações da
solução, é esperado que esses vetores de descida corrigidos deem ainda uma melhor
solução.

Comecemos fazendo com que \(d_1 = -\nabla f(x_1) = -g_1 \), já que isso pode nos por na
direção correta do mínimo. O algoritmo, sendo iterativo, tem uma forma semelhante ao
gradiente descendente, mudando apenas a atualização:

\begin{equation}
\label{att_gc}
x_{k+1} = x_k + \alpha_k d_k
\end{equation}

Onde \(d_k \in U\) e \(\alpha_k\) é o tamanho do passo, que pode ser exato minimizando
\(f\) na direção \(d_k\).

Como queremos direções conjugadas, que por sua vez são geradas a partir dos gradientes,
então o gradiente no próximo ponto já pode ser conjugado em relação à direção atual.
Considerando (\ref{att_gc}), podemos dizer que:

\begin{equation}
  \label{def_gc_gk1}
  \nabla f(x_{k+1}) = g_{k+1} = g_k + \alpha_k Ad_k
\end{equation}

Com isso, queremos que:

\begin{equation}
\label{def_gc_dk_gk1}
\langle g_{k+1}, d_k \rangle = 0 \Rightarrow \langle g_k + \alpha_k A d_k, d_k \rangle = 0
\end{equation}


Considerando as propriedades inerentes a um produto interno:

\begin{equation}
\langle g_k, d_k \rangle + \alpha_k \langle A d_k, d_k \rangle = 0 \Rightarrow \alpha_k \langle A d_k, d_k \rangle = - \langle g_k, d_k \rangle
\end{equation}

Por fim, resolvendo para \(\alpha_k\) e mudando a notação:

\begin{equation}
\alpha_k= - \frac{\langle g_k, d_k \rangle}{\langle A d_k, d_k \rangle} \Rightarrow \alpha_k= - \frac{\langle g_k, d_k \rangle}{\langle d_k, d_k \rangle_A}
\end{equation}


O que nos falta agora é a atualização de \(d_k\). Como dito anteriormente, queremos
construir essas direções a partir do gradiente negativo corrigido para que seja
A-ortogonal em relação às direções já encontradas.

\begin{equation}
\label{def_dk1}
d_{k+1} = -g_{k+1} + \beta_k d_k
\end{equation}

Portando, \(\beta_k\) deve ser escolhido de forma que \(\langle d_{k+1}, d_k \rangle_A = 0\):

\begin{equation}
\langle -g_{k+1} + \beta_k d_k, d_k \rangle_A = 0 \Rightarrow \langle -g_{k+1}, d_k \rangle_A + \beta_k \langle d_k, d_k \rangle_A = 0
\end{equation}

Rearranjando e resolvendo para \( \beta_k \):

\begin{equation}
  \beta_k \langle d_k, d_k \rangle_A = - \langle -g_{k+1}, d_k \rangle_A \Rightarrow \beta_k = -\frac{\langle -g_{k+1}, d_k \rangle_A}{\langle d_k, d_k \rangle_A}
\end{equation}

Já temos o algoritmo pronto, mas ainda é custoso. Com mais um pouco de trabalho, podemos
remover duas multiplicações por A de \(\beta_k\):

\begin{equation}
\label{def_beta_gc}
\beta_k = - \frac{\langle -g_{k+1}, Ad_k \rangle}{\langle d_k, Ad_k \rangle}
\end{equation}

Queremos substituir \(Ad_k\) por algo mais simples, para isso podemos usar (\ref{def_gc_gk1}):

\begin{equation}
\label{def_adk}
g_{k+1} = g_k + \alpha_k A d_k \Rightarrow g_{k+1} - g_k = \alpha_k A d_k \Rightarrow A d_k = \frac{g_{k+1} - g_k}{\alpha_k}
\end{equation}

Como \(\beta_k\) é definido por uma razão, podemos trabalhar o numerador e o denominador
separadamente. Então comecemos trabalhando apenas o numerador de \(\beta_k\):

\begin{equation}
\langle -g_{k+1}, Ad_k \rangle
\end{equation}

Substituindo \(Ad_k\) pela definição em (\ref{def_adk}), e extraindo os coeficientes:

\begin{equation}
\langle -g_{k+1}, \frac{g_{k+1} - g_k}{\alpha_k} \rangle \Rightarrow -\frac{1}{\alpha_k} \langle g_{k+1}, g_{k+1} - g_k \rangle
\end{equation}

E pela linearidade do produto interno:

\begin{equation}
-\frac{1}{\alpha_k}(\langle g_{k+1}, g_{k+1} \rangle + \langle -g_{k+1}, - g_k \rangle)
\end{equation}


Sabemos, porque construímos assim, que \(\langle g_{k+1}, g_k \rangle = 0\). Por fim, temos que o
numerador de \(\beta_k\) é dado por:

\begin{equation}
\label{def_gc_numerador}  
-\frac{1}{\alpha_k} \langle g_{k+1}, g_{k+1} \rangle
\end{equation}

Agora trabalhando o denominador de \(\beta_k\):

\begin{equation}
\langle d_k, Ad_k \rangle
\end{equation}


Novamente, usando (\ref{def_adk}), e extraindo o coeficiente:

\begin{equation}
\langle d_k, \frac{g_{k+1} - g_k}{\alpha_k} \rangle \Rightarrow \frac{1}{\alpha_k}\langle d_k, g_{k+1} - g_k \rangle
\end{equation}

Pela linearidade do produto interno e usando (\ref{def_gc_dk_gk1}):

\begin{equation*}
\frac{1}{\alpha_k} (\langle d_k, g_{k+1} \rangle + \langle d_k, - g_k \rangle) \Rightarrow \frac{1}{\alpha_k} \langle d_k, - g_k \rangle
\end{equation*}

Considerando (\ref{def_dk1}) para substituir \(d_k\):

\begin{equation}
\frac{1}{\alpha_k} \langle -g_{k} + \beta_{k-1} d_{k-1}, - g_k \rangle
\end{equation}

Aplicando a linearidade e extraindo coeficientes:


\begin{equation}
\frac{1}{\alpha_k}(
\langle -g_{k}, - g_k \rangle +
\langle \beta_{k-1} d_{k-1}, - g_k \rangle
) \Rightarrow
\frac{1}{\alpha_k}(
\langle -g_{k}, - g_k \rangle -
\beta_{k-1}(\langle d_{k-1}, g_k \rangle)
)
\end{equation}

Por fim, considerando (\ref{def_gc_dk_gk1}), temos o denominador dado por:

\begin{equation}
\label{def_gc_denominador}
\frac{1}{\alpha_k} \langle -g_{k}, - g_k \rangle
\end{equation}


Finalmente, podemos reescrever \(\beta_k\). Considerando (\ref{def_beta_gc}), (\ref{def_gc_numerador}) e (\ref{def_gc_denominador}), e substituindo:

\begin{equation}
\beta_k = - \frac{\langle -g_{k+1}, Ad_k \rangle}{\langle d_k, Ad_k \rangle} \Rightarrow \beta_k = - \frac{-\frac{1}{\alpha_k} \langle g_{k+1}, g_{k+1} \rangle}{\frac{1}{\alpha_k} \langle -g_{k}, - g_k \rangle}
\end{equation}

Isolando e resolvendo o coeficiente:

\begin{equation}
  \beta_k = - \frac{-\frac{1}{\alpha_k}}{\frac{1}{\alpha_k}} \cdot \frac{\langle g_{k+1}, g_{k+1} \rangle}{\langle -g_{k}, - g_k \rangle}   \Rightarrow \beta_k = - (-1) \frac{\langle g_{k+1}, g_{k+1} \rangle}{\langle -g_{k}, - g_k \rangle}
\end{equation}

Por fim, temos a última peça para a definição do algoritmo do gradiente conjugado:

\begin{equation}
\beta_k = \frac{\langle g_{k+1}, g_{k+1} \rangle}{\langle -g_{k}, - g_k \rangle}
\end{equation}

Em resumo, iniciando com \(x_1\) sendo uma estimativa do ótimo e a direção inicial sendo \(d_1 = -\nabla f(x_1) = -g_1\), temos que o algoritmo é:

\begin{equation}
x_{k+1} = x_k + \alpha_k d_k
\end{equation}

\begin{equation}
\alpha_k= - \frac{\langle g_k, d_k \rangle}{\langle d_k, d_k \rangle_A}
\end{equation}

\begin{equation}
d_{k+1} = -g_{k+1} + \beta_k d_k
\end{equation}

\begin{equation}
\beta_k = \frac{\langle g_{k+1}, g_{k+1} \rangle}{\langle -g_{k}, - g_k \rangle}
\end{equation}

Podemos ver que o método não é complexo de ser usado, nem tão custoso computacionalmente em casos simples.
Em problemas de maior dimensão, funções custosas ou ainda problemas difíceis que não sejam quadráticos, o
método pode não ser o recomendado pelo fato de considerar várias avaliações da função para uma única iteração.