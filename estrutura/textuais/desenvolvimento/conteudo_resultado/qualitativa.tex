\section{Análise Qualitativa do algoritmo - SCP}

O SCP resolve problemas no seguinte formato:

\vspace{-15pt}
\begin{mini!}
{x}{ f(x) \label{scp_obj}}{\label{prob_scp}}{}
\addConstraint{g_j(x)}{\leq 0, \quad}{j = 1, ..., m_i \label{r1_scp}}
\addConstraint{h_r(x)}{= 0, \quad}{r = 1, ..., m_e \label{r2_scp}}
\end{mini!}

Onde todas as funções devem ser continuamente diferenciáveis em \(\mathbb{R}^n\) assim como
devem ser funções \( \mathbb{R}^n \mapsto \mathbb{R} \). Nenhuma função é considerada convexa e isto
pode ser visto como um avanço à forma anterior do método. A única coisa assumida é que alguma das
funções \( g_j(x) \) é linear. Com isso podemos perceber que o SCP é capaz de trabalhar com uma
grande quantidade de problemas, visto que essa forma de problema pode ser modelada de aplicações
em diversas áreas.

Outra coisa que é assumida, é que as qualificações de restrições de Mangasarian-Fromovitz estendidas (EMFCQ)
\cite{di1994exact} são válidas para qualquer \( x \in \mathbb{R}^n \), como visto na seção \ref{sec_emfcq}.

Estas qualificações garantem que, para qualquer ponto não viável, exista uma
direção de descida em direção a uma região viável. Existe também uma relação entre as EMFCQ
e funções de penalidade exatas.


\subsection{O Algoritmo}
O algoritmo é iterativo, assim como no clássico método de Newton se é gerada
uma sequência de pontos convergentes para um ponto ótimo.
Cada uma de suas iterações consiste em um conjunto de subiterações de
resolução de problemas lineares. A cada problema linear resolvido, é obtida
uma direção de descida, e com essa direção, é feita uma busca em linha,
buscando um tamanho ótimo para o passo. Após essas iterações de problemas
lineares, é esperado que um novo ponto capaz de reduzir suficientemente uma
função de mérito, o que garante a convergência.



\begin{algorithm}[H]
  \SetAlgoLined
  \( k \gets 1 \)\;
  \( H_k \gets I \)\;
  \( x_k \gets \) ponto inicial\;
  
  \While{k \(\leq\) Iterações máximas}{
    \((d_k, x_{k+1}, H_{k+1}, \lambda_{k+1}, \mu_{k+1}, est) \gets SPL(x_k, H_k) \) \tcp{Problemas lineares}

    \If{est = true} {
      break \tcp*{Para se foi encontrado um ponto KKT estacionário}
    }

    \(x_{k+1} \gets VFM(d_k, x_k, \lambda_k, \mu_k, x_{k+1}, \lambda_{k+1}, \mu_{k+1})\) \tcp*{Verificação de descida}
    \(ARC(x_k, x_{k+1})\)\tcp*{Atualização das regiões de confiança}
    
    \If{\(KKTE(x_{k+1}, \lambda_{k+1}, \mu_{k+1})\) = true} {
      break \tcp*{Para se foi encontrado um ponto KKT estacionário}
    }
    
    \(k \gets k + 1\)

  }
  \caption{SCP}
\end{algorithm}
\vspace{15pt}


A função \(SPL\) é responsável por resolver a sequência de problemas lineares
buscando direções ótimas de descida enquanto já desloca o ponto atual. A função
\(VFM\) é responsável por fazer a verificação da qualidade do novo ponto encontrado
por \(SPL\), se o ponto satisfizer as condições impostas, ele é mantido, caso contrário
é buscado um que satisfaça por outros meios. A função \(ARC\) verifica o tamanho da região
de confiança para as soluções \(d\) encontradas em \(SPL\). Por fim, \(KKTE\) verifica
se o ponto é um KKT estacionário do problema. Quando o algoritmo para, o ponto
mais recente adicionado à sequência é a solução, \(x_k\) se foi atingido
o número máximo de iterações, ou \(x_{k+1}\) em uma parada por ponto
estacionário.

\subsubsection{Subiterações lineares}

Cada subiteração linear é a resolução de um problema linear fixado no ponto
atual \( x_i \), \(i\) sendo a iteração linear atual. O problema linear que
será resolvido é dado por:


\vspace{-15pt}
\begin{mini!}
{x}{   \nabla f(x_i)^T d + C(\sum_{j=1}^{m_i} t_j^g + \sum_{r=1}^{m_e} t_r^{h^+} + \sum_{r=1}^{m_e} t_r^{h^-}) \label{scpl_obj}}{\label{prob_scpl}}{}
\addConstraint{g_j(x_i) + \nabla g_j(x_i)^T d}{ \leq t_j^g, \quad}{j = 1, ..., m_i \label{r1_scpl}}
\addConstraint{h_r(x_i) + \nabla h_r(x_i)^T d}{= t_r^{h^+} - t_r^{h^-}, \quad}{r=1, ..., m_e \label{r2_scpl}}
\addConstraint{(d_r)^T (H_i d)}{= 0, \quad}{r=1, ..., i-1, i > 1 \label{r3_scpl}}
\addConstraint{d^L \leq d}{\leq d^U}{ \label{r4_scpl}}
\addConstraint{0 \leq t_j^g}{\leq max(g_j(x_i), 0), \quad}{j=1, ..., m_i \label{r5_scpl}}
\addConstraint{0 \leq  t_r^{h^+}}{\leq |h_r(x_i)|, \quad}{r = 1, ..., m_e \label{r6_scpl}}
\addConstraint{0 \leq  t_r^{h^-}}{\leq |h_r(x_i)|, \quad}{r = 1, ..., m_e \label{r7_scpl}}
\end{mini!}
com a solução sendo \( d, t^g, t^{h^+}, t^{h^-} \).

As restrições (\ref{r1_scpl}) e (\ref{r2_scpl}) são versões lineares das restrições originais,
onde a busca por um \(d_i \) que resolva o problema garante que na próxima iteração, o ponto já não
viole restrições. As variáveis \(t^g\), \(t^{h^+}\), \(t^{h^-}\), são relaxamentos das restrições,
o que facilita encontrar um \(d_i\) dentro de (\ref{r4_scpl}).

A restrição (\ref{r3_scpl}) faz com que a direção de descida \( d_i \) seja H-conjugada (\ref{aortogonal}) à todas
as outras direções encontradas anteriormente por outras subiterações lineares desta
interação não linear. \( H_i \) é a matriz hessiana da função Lagrangiana (\ref{fn_lagrangiana}) do problema
original, também avaliada no ponto \( x_i \).

Não é necessária mais que uma iteração de resolução de problema linear para
que o algoritmo venha a convergir, mas ainda assim pode ser feito para melhorar a velocidade
de convergência. Como é necessário que a cada iteração linear a solução
\( d_i \) seja conjugada à todos os outros \(d\)s de iterações lineares anteriores, o
algoritmo busca uma direção de descida que "aponte" mais certeiramente para o ótimo.
Por esse motivo esse método lembra o gradiente conjugado, quando essa restrição é
considerada.

A restrições (\ref{r4_scpl}) nada mais são que um paralelepípedo que limita o espaço de busca de uma
direção descida. Onde \( d^L < \overrightarrow 0 \) e \( d^U > \overrightarrow 0 \).

As linearizações vistas em (\ref{r1_scpl}) e (\ref{r2_scpl}) e o requerimento de ortogonalidade entre
as direções de descida encontradas em respeito à Lagrangiana da função em (\ref{r4_scpl}) são (hiper)planos
que cortam o espaço.

As restrições (\ref{r1_scpl}) são linearizações que geram uma região no espaço que aproxima a região viável
das restrições \( g_j(x), j=1, ..., m_i \). Da mesma forma (\ref{r2_scpl}) são (hiper)planos que aproximam
também a região limitada de \( h_r(x), r=1, ..., m_e \).

Por fim as restrições (\ref{r5_scpl}), (\ref{r6_scpl}) e (\ref{r7_scpl}) controlam o quanto a função objetivo
(\ref{scpl_obj}) é relaxada em relação às restrições que não estão sendo satisfeitas. Quando todas as restrições
são satisfeitas em \(x_i\), vemos que \( t^g \) ,\(t^{h^+}\) e \(t^{h^-} \) são todos nulos.


Para a implementação da função \(SPL\) precisamos inicialmente de uma forma de gerar o problema linear
(\ref{prob_scpl}), que uma vez gerado, deve ser resolvido. O próximo passo a ser tomado é resolver
o problema dual de (\ref{prob_scpl}), uma vez que a solução do problema dual pode ser usada como estimativa
dos multiplicadores de Lagrange. Após isso, já temos a possibilidade de verificar se o ponto atual é um
ponto KKT estacionário (visto a necessidade dos multiplicadores de Lagrange para isso). Caso seja KKT
estacionário já podemos retornar as informações necessárias, do contrário, é feita uma busca em linha
buscando um tamanho para o passo na direção encontrada na solução do problema linear. Essa busca em
linha é feita em uma função Lagrangiana penalizada de acordo com as restrições cumpridas. Uma vez
encontrado o tamanho do passo, o ponto da interação atual pode ser atualizado como no método de gradiente
descendente. Agora, a depender de como foi escolhido, é computada a matriz Hessiana da função Lagrangiana
para a próxima iteração. Por fim, caso algum dos critérios de parada de subiterações lineares forem
satisfeitos, retornamos as informações obtidas até agora, caso contrário, repetimos o processo tomando
o ponto atualizado como o novo ponto de partida.


\vspace{15pt}
\begin{algorithm}[H]
  \SetAlgoLined
  \SetKwFunction{FSPL}{SPL}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\FSPL{\(x_k, H_k\)}}{
    \(i \gets 1\)\;
    \(x_i \gets x_k\)\;
    \(H_i \gets H_k\)\;
      
  \While{Condições de parada não satisfeitas}{
    \( (A, b, c) \gets gerar\_problema(x_i, H_i) \)\;
    \(( d_i, t^g_i, t^{h^+}_i, t^{h^-}_i) \gets primal(A, b, c) \)\;
    \((\lambda_i, \mu_i)  \gets dual(A, b, c)\)\;
    
    \If{\(KKTE(x_i, \lambda_i, \mu_i)\) = true} {
      \KwRet \((d_i, x_i, H_i, \lambda_i, \mu_i, true)\)\;
    }

    \(\alpha_i \gets busca\_em\_linha\_lagrangiana\_penalizada(x_i, d_i, \lambda_i, \mu_i)\)\;
    \(x_{i+1} \gets x_i + \alpha_i d_i \)\;
    \(H_{i+1} \gets hessiana(x_i, x_{i+1})\)\;
    \(i \gets i + 1\)\;
  }
  
  \KwRet \((d_i, x_i, H_i, \lambda_i, \mu_i, false)\)\;
  }
  \caption{SPL}
\end{algorithm}
\vspace{15pt}


Para a geração e resolução do problema é necessário compreender o formato dele e ajusta-lo
para a melhor forma possível de resolver. Para isso, vamos considerar o seguinte problema
de minimização linear e seu dual:

\begin{mini!}
{x}{ c^Tx}{\label{prob_linear_pdr}}{}
\addConstraint{Ax}{\geq b}{\label{dr12}}
\end{mini!}

\begin{maxi!}
{y}{ b^Ty}{}{}
\addConstraint{A^Ty}{= c}{\label{dr12}}
\end{maxi!}

Para que possamos resolver (\ref{prob_scpl}) com facilidade pelos métodos convencionais,
temos que transforma-lo na tripla \(A, b, c\) dos problemas acima, e com isso, também
considerar apenas um único vetor solução \(x\). Tomemos \(x\) como sendo
\((d, t^g, t^{h^+}, t^{h^-})\), logo, um vetor do \(\mathbb{R}^{n + m_i + m_e + m_e}\).
Podemos perceber rapidamente o que é o vetor \(c\) a partir de (\ref{scpl_obj}),
que as primeiras entradas são \(\nabla f(x_i)\) e todo o resto é o escalar de
relaxamento \(C\). Para a matriz \(A\) e o vetor \(b\) precisamos primeiro
reescrever as restrições de forma que fique clara as entradas. Podemos reescrever
(\ref{prob_scpl}) da seguinte forma:

\vspace{-15pt}
\begin{mini!}
{x}{   \nabla f(x_i)^T d + \sum_{j=1}^{m_i}Ct_j^g + \sum_{r=1}^{m_e} Ct_r^{h^+} + \sum_{r=1}^{m_e} Ct_r^{h^-} \label{scplt_obj}}{\label{prob_scplt}}{}
\addConstraint{ -\nabla g_j(x_i)^T d +  e_j^T  t^g + 0 \cdot t^{h^+} + 0 \cdot t^{h^-}  }{\geq          g_j(x_i), \quad}{j = 1, ..., m_i \label{r1_scplt}}
\addConstraint{ -\nabla h_r(x_i)^T d + 0 \cdot t^g + e_r^T   t^{h^+} - e_r^T   t^{h^-}  }{\geq          h_r(x_i), \quad}{r = 1, ..., m_e \label{r2_scplt}}
\addConstraint{  \nabla h_r(x_i)^T d + 0 \cdot t^g - e_r^T   t^{h^+} + e_r^T   t^{h^-}  }{\geq         -h_r(x_i), \quad}{r = 1, ..., m_e \label{r3_scplt}}
\addConstraint{          -(Hd_r)^T d + 0 \cdot t^g + 0 \cdot t^{h^+} + 0 \cdot t^{h^-}  }{\geq                 0, \quad}{r = 1, ..., i-1 \label{r4_scplt}}
\addConstraint{           (Hd_r)^T d + 0 \cdot t^g + 0 \cdot t^{h^+} + 0 \cdot t^{h^-}  }{\geq                 0, \quad}{r = 1, ..., i-1 \label{r5_scplt}}
\addConstraint{           1 \cdot  d + 0 \cdot t^g + 0 \cdot t^{h^+} + 0 \cdot t^{h^-}  }{\geq               d^L       }{                \label{r6_scplt}}
\addConstraint{          -1 \cdot  d + 0 \cdot t^g + 0 \cdot t^{h^+} + 0 \cdot t^{h^-}  }{\geq              -d^U       }{                \label{r7_scplt}}
\addConstraint{           0 \cdot  d + e_j^T   t^g + 0 \cdot t^{h^+} + 0 \cdot t^{h^-}  }{\geq                 0, \quad}{j = 1, ..., m_i \label{r8_scplt}}
\addConstraint{           0 \cdot  d - e_j^T   t^g + 0 \cdot t^{h^+} + 0 \cdot t^{h^-}  }{\geq -max(0, g_j(x_i)), \quad}{j = 1, ..., m_i \label{r9_scplt}}
\addConstraint{           0 \cdot  d + 0 \cdot t^g + e_r^T   t^{h^+} + 0 \cdot t^{h^-}  }{\geq                 0, \quad}{r = 1, ..., m_e \label{r10_scplt}}
\addConstraint{           0 \cdot  d + 0 \cdot t^g - e_r^T   t^{h^+} + 0 \cdot t^{h^-}  }{\geq       -|h_r(x_i)|, \quad}{r = 1, ..., m_e \label{r11_scplt}}
\addConstraint{           0 \cdot  d + 0 \cdot t^g + 0 \cdot t^{h^+} + e_r^T   t^{h^-}  }{\geq                 0, \quad}{r = 1, ..., m_e \label{r12_scplt}}
\addConstraint{           0 \cdot  d + 0 \cdot t^g + 0 \cdot t^{h^+} - e_r^T   t^{h^-}  }{\geq       -|h_r(x_i)|, \quad}{r = 1, ..., m_e \label{r13_scplt}}
\end{mini!}
em que \(e_k\) é a \(k\)-ésima base canônica e \((\cdot)\) é o produto de um vetor por um escalar.

Posto assim, extrair \(A\) e \(b\) não é tão difícil. Basta considerar funções para
cada família de restrições retornando blocos de \(A\) e entradas de \(b\). Durante
esse processo é necessário tomar cuidado com o custo tanto em memória consumida
quanto em processamento. Todas as funções e restrições são avaliadas em \(x_i\),
então é correto que se faça essa avaliação antes de começar a gerar a matriz e os
vetores. A respeito da memória, não há muito o que se possa fazer, a não ser
considerar variáveis temporárias ou algo semelhante.

Depois de gerar a matriz e os vetores, chega a vez de resolver o problema. A forma
como o problema é resolvido não importa, desde que se obtenha o resultado correto
de forma eficiente. Uma vez que a linguagem escolhida para a implementação foi Rust,
foi usada uma biblioteca própria para problemas lineares, chamada de minilp, mantida
por Alexey Zatelepin \cite{minilp}, que é capaz de resolver os problemas rapidamente.

Resolvido ambos os problemas, primal e dual, precisamos agora extrair as estimativas
dos multiplicadores de Lagrange. Para isso foi considerada a prova do Lema 3 em \cite{Still2010}
onde essa versão do SCP foi proposta. Pela prova podemos perceber que as estimativas dos
multiplicadores são as \(m_i + m_e + m_e\) entradas da solução do problema dual, referentes
às restrições (\ref{r1_scplt}), (\ref{r2_scplt}) e (\ref{r3_scplt}). Podemos extrair
\(\lambda\) das \(m_i\) primeiras entradas da solução dual. Já para \(\mu\), precisamos
ter \(\mu^+\) como as \(m_e\) entradas logo após \(m_i\) entradas, e  \(\mu^-\) como as
\(m_e\) entradas logo após \(m_i+m_e\) entradas, e por fim, \(\mu = \mu^+ - \mu^-\).

Agora com as estimativas dos multiplicadores podemos verificar se o ponto atual é
um KKT estacionário. Caso seja, já podemos retornar as informações pertinentes,
\(d_i, x_i, H_i, \lambda_i, \mu_i\) e um sinal informando que as iterações podem
parar. Esse sinal é mais necessário por questões de eficiência, se não fosse
enviado um sinal o algoritmo pararia na próxima verificação de KKT, no entanto,
seriam feitas uma imensa quantidade de operações à toa.

O próximo passo é encontrar um tamanho de passo na direção \(d_i\) que minimize
uma função Lagrangiana penalizada, dada por:

{\footnotesize
\begin{equation}
  \label{scp_langrangiana_penalizada}
  \widetilde{L}(x, \lambda, \mu) = f(x) + \sum_{j=1}^{m_i} \lambda_j g_j(x)^+ + \sum_{r=1}^{m_e}|\mu_rh_r(x)| + \rho \sum_{j=1}^{m_i} \lambda_j (g_j(x)^+)^2 + \rho  \sum_{r=1}^{m_e}|\mu_r|(h_r(x))^2
\end{equation}
}

Podemos perceber essa função como a Lagrangiana penalizada já que ela considera apenas as
restrições que não foram cumpridas, uma vez que \(g_j(x)^+ = max(0, g_j(x))\). Como se
deseja minimizar essa função na direção \(d_i\), qualquer restrição que venha a ser
descumprida irá fazer a função crescer. O objetivo da minimização é encontrar um
único valor real \(\alpha_i\) entre \(0\) e \(1\).

\begin{equation}
  \underset{0\leq \alpha_i \leq 1}{\mathrm{min}}\hspace{0.2cm}  \widetilde{L}(x_i + \alpha_i d_i, \lambda_i, \mu_i)
\end{equation}

A forma como é feita essa busca em linha deve ser escolhida com cuidado. Se
for possível e de interesse, uma busca em linha exata deve ser feita, se não
pelo menos deve-se considerar uma boa aproximação. Se o tamanho do passo for
impreciso demais, podemos ter problemas quando o algoritmo estiver próximo
de um ótimo, podendo, por exemplo, nunca se aproximar o suficiente para
que seja considerado solução.

Um próximo passo, se foi escolhido fazer dessa forma, é a computação
da matriz Hessiana da função (\ref{fn_lagrangiana}) Lagrangiana. Os autores do
SCP propuseram o uso do BFGS (\ref{sec_bfgs}) por ser um método iterativo, e
por isso, poder ser evitado cálculos desnecessários. A outra opção considerada
para a implementação foi calcular a Hessiana de forma exata, usando o método
já apresentado em (\ref{diffinitah}).




Agora, no final da iteração, é incrementado o contador de iterações e são verificadas
as condições de parada. Temos ao todo cinco condições a serem verificadas, bastando
que alguma delas seja descumprida para parar as iterações. A primeira verifica se
\(i > n\), aplicável apenas quando a restrição (\ref{r3_scpl}) é considerada, já
que sendo um método conjugado, não se tem mais como haver outras direções de descida
para a busca da solução. A segunda é quando a norma de \(d_i\) é próxima de 0, indicando
nenhum movimento. A terceira condição é quando alguma variável de relaxamento não é
reduzida, ou seja, não existe nenhum \(j\) de forma que \(t^g_j \leq g_j(x)\) e
também não existe nenhum \(r\) de forma que
\(t^{h^+}_r \leq h_r(x)\) e  \(t^{h^+}_r \leq h_r(x)\). A quarta condição é se já estamos
pelo menos na segunda iteração e qualquer variável de relaxamento é positiva. A quinta
é se \(\alpha_i\) é próximo de 0, pelos mesmos motivos que a segunda condição. Uma vez
parada as iterações de resoluções de problemas lineares, deve ser retornada as informações
necessárias, \((d_i, x_i, H_i, \lambda_i, \mu_i)\).

\subsubsection{Função Mérito}
É usada uma função de mérito para verificações da velocidade de descida. O mérito é dado pela
minimização da função objetivo (\ref{scp_obj}) e a anulação de um termo de penalização:

\begin{equation}
  M(x) = f(x) + p(x)
\end{equation}


O termo de penalização é semelhante aos dois primeiros termos que penalizam a função objetivo
na Lagrangiana penalizada (\ref{scp_langrangiana_penalizada}). No entanto, ao invés de ser
usado as estimativas dos multiplicadores de Lagrange, são usados números maiores que elas,
garantindo que a penalização seja ditada apenas por uma proporção das restrições.


\begin{equation}
 p(x) = \sum_{j=1}^{m_i} \bar{\lambda_j} g_j(x)^+ + \sum_{r=1}^{m_r} \bar{\mu_r} |h_r(x)|
\end{equation}


Não é dito o quanto os valores para \(\bar{\lambda}\) e \(\bar{\mu}\) devem ser maiores que
as estimativas, então devemos contar com a experiência trazida pela aplicação do método.
Já que \(\bar{\lambda} > \lambda\) e \(\bar{\mu} > \mu\) basta considerar um incremento
para os valores, \(\bar{\lambda} = \lambda + \lambda_{inc}\) e  \(\bar{\mu} = \mu + \mu_{inc}\).


A verificação é dada por três condições, o decréscimo da função de mérito, o decréscimo
suficiente da função de mérito, e o passo tomado é suficientemente grande. A primeira
condição é apenas verificar se \(M(x_{k+1}) \leq M(x_k)\). A segunda condição é feita
por querer descobrir se o passo tomado desacelera a descida do algoritmo, verificando
se a função está sendo reduzida suficientemente. Essa verificação é feita pela análise
da diferença das avaliações da função de mérito em \(x_k\) e \(x_{k+1}\) em relação
com a derivada direcional da função.

\begin{equation}
  M(x_{k+1}) - M(x_k) \leq \sigma \alpha_k D_{d_k} M(x_k), \hspace{0.2cm} \sigma \in (0, 1)
\end{equation}

Uma interpretação rápida para a derivada direcional é medir o quando a descida pelo
gradiente vai de encontro com a direção esperada. Nesse caso, a derivada mede a velocidade
de descida da função de mérito na direção \(d\). A derivada direcional de \(M(x)\) é dada
por:

\begin{equation}
  D_d M(x) = \nabla f(x)^Td + \sum_{j=1}^{m_i} \bar{\lambda} D_dg_j(x)^+ +  \sum_{r=1}^{m_e} \bar{\mu} D_d|h_r(x)|
\end{equation}


E as derivadas direcionais das restrições consideram apenas quando as restrições não estão sendo
cumpridas:

\begin{equation}
  D_dg_j(x)^+ = \begin{cases}
    \nabla g_j(x)^Td,         & \text{se } g_j(x) > 0;\\
    max(\nabla g_j(x)^Td, 0), & \text{se } g_j(x) = 0;\\
    0,                       & \text{se } g_j(x) < 0.
  \end{cases}
\end{equation}


\begin{equation}
  D_d|h_r(x)| = \begin{cases}
    \nabla h_r(x)^Td,         & \text{se } h_r(x) > 0;\\
    |\nabla h_r(x)^Td|,       & \text{se } h_r(x) = 0;\\
    -\nabla h_r(x)^Td,        & \text{se } h_r(x) = 0.
  \end{cases}
\end{equation}

O terceiro teste com a função de mérito é a verificação de que o passo tomado foi grande o
suficiente. Essa condição é feita por verificar se a derivada direcional em \(x_{k+1}\) é
maior que em \(x_k\), mesmo que reduzida:

\begin{equation}
  D_{d_k} M(x_{k+1}) \geq \eta D_{d_k} M(x_k), \hspace{0.2cm} \eta \in (\sigma, 1)
\end{equation}


Como a derivada direcional é maior quanto mais próximo de \(d\), visto que o movimento descrito
por \(x_{k+1} - x_k\) caracteriza o denominador da derivada, essa condição ser satisfeita
significa que as iterações estão cada vez melhor "apontadas" para o ótimo e fazendo isso
com uma velocidade significante.

A derivada direcional da função de mérito, além de considerar a descida da função objetivo,
também considera as direções de descida para que as restrições que não estão sendo cumpridas
sejam. Então, de forma geral, a derivada direcional da função de mérito quantifica o quão
próximo da melhor descida possível o algoritmo está fazendo.

Caso não seja possível cumprir todas as três condições, é necessário buscar um novo ponto
que satisfaça. Dentre as várias formas de se fazer isso, a mais simples é buscar o ponto
na direção \(d_k\) que minimize a função de mérito:

\begin{equation}
\underset{\alpha_n}{\mathrm{min}} \hspace{0.3cm} M(x_k + \alpha_n d_k)
\end{equation}

Uma vez encontrado \(\alpha_n\), basta salvar o novo ponto em \(x_{k+1}\) e retornar:

\begin{equation}
  x_{k+1} = x_k + \alpha_n d_k
\end{equation}

Um pseudo-código para o que foi dito é:

\vspace{15pt}
\begin{algorithm}[H]
  \SetAlgoLined
  \SetKwFunction{FVFM}{VFM}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\FVFM{\(d_k, x_k, \lambda_l, \mu_k, x_{k+1}, \lambda_{k+1}, \mu_{k+1}\)}}{
    \(\sigma \gets 0.5 \)\;
    \(\eta \gets 0.75 \)\;
    \(\alpha_k \gets \frac{x_{k+1} - x_k}{d_k} \)\;
    \(c1 \gets (M(x_{k+1}) \leq M(x_k) )\)\;
    \(c2 \gets ( M(x_{k+1})-M(x_k) \leq \sigma \alpha_k D_{d_k}M(x_k) )\)\;
    \(c3 \gets ( D_{d_k}M(x_{k+1}) \geq \eta D_{d_k}M(x_k) )\)\;
    \If{\(c1\) = true e \(c2\) = true e \(c3\) = true} {
      \KwRet \(x_{k+1}\)\;
    }
    \(\alpha_k \gets busca\_em\_linha\_funcao\_merito(x_k, d_k, \lambda_k, \mu_k) \)\;
    \(x_{k+1} \gets x_k + \alpha_k d_k \)\;
    \KwRet \(x_{k+1}\)\;
  }
  \caption{VFM}
\end{algorithm}
\vspace{15pt}



\subsubsection{Regiões de confiança}
As regiões de confiança são usadas como delimitações para o espaço das direções de descidas,
garantindo que estas sejam de um tamanho adequado, e atualizadas de acordo com os movimentos do
ponto em cada iteração, ou seja, se a região é muito grande e o ponto em relação a última
iteração não se deslocou tanto, a região pode ser diminuída, e o mesmo para o contrário,
a região é aumentada caso a diferença de posições entre iterações seja muito grande. O
parâmetro é um número real \(\delta_{max}\) dado por:

\begin{equation}
  \delta_{max} = max(\delta_l)
\end{equation}

Onde \(\delta_l\) é um conjunto de valores reais das maiores diferenças em relação ao
tamanho da região de confiança:

\begin{equation}
  \delta_l = max \left ( \left | \frac{x_{k+1, l} - x_{k, l}}{d^L_l} \right |, \left | \frac{x_{k+1, l} - x_{k, l}}{d^U_l} \right | \right ), \hspace{0.2cm} l = 1, ..., n
\end{equation}

Existe uma tolerância para o tamanho do movimento sem que as regiões sejam atualizadas.
Um parâmetro é escolhido como limite antes de crescer a região e outro para diminuir a
região. Por experiência numérica dos autores, foram identificados valores para esses
parâmetros, para o crescimento da região \(\delta_{inc} = 0.8\) e para a diminuição
\(\delta_{dec} = 0.2\). Podem ser escolhidos quaisquer valores dentro dos intervalos,
que são \(0.5 \leq \delta_{inc} \leq 1\) e \(0 \leq \delta_{dec} \leq 0.5\). Uma
vez considerado esses parâmetros a atualização pode ser feita pelo seguinte:

\vspace{15pt}
\begin{algorithm}[H]
  \SetAlgoLined
  \SetKwFunction{FARC}{ARC}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\FARC{\(x_k, x_{k+1}\)}}{
    \(\delta_l \gets \{\}\)\;

    \(\delta_{inc} \gets 0.8\)\;
    \(\delta_{dec} \gets 0.2\)\;

    \For{\(l \gets 1\) \KwTo \(n\)}{
      \(\delta \gets x_{k+1, l} - x_{k, l}\)\;
      \(\delta^L \gets \frac{\delta}{d^L_l}\)\;
      \(\delta^U \gets \frac{\delta}{d^U_l}\)\;
      \(\delta_l \gets \delta_l \cup \{\mathrm{max}(\delta^L, \delta^U)\} \)\;
    }

    \( \delta_{max} = max(\delta_l) \)\;
        
    \If{\(\delta_{max} < \delta_{dec}\)} {
      \(\gamma \gets \frac{\delta_{max}}{\delta_{dec}}\)\;
      \If{\(\gamma \ne 0\)} {
        \( d^L \gets \gamma d^L  \)\;
        \( d^U \gets \gamma d^U  \)\;        
      }
    }


    \If{\(\delta_{max} > \delta_{inc}\)} {
      \(\gamma \gets 2\delta_{max}\)\;
      \If{\(\gamma \ne 0\)} {
        \( d^L \gets \gamma d^L  \)\;
        \( d^U \gets \gamma d^U  \)\;        
      }
    }
  }
  \caption{ARC}
\end{algorithm}
\vspace{15pt}

Algumas coisas precisam ser consideradas a respeito dessa atualização. Embora seja
uma função de atualização simples e não muito custosa, não dar atenção suficiente
pode resultar em problemas. A verificação \(\gamma \ne 0\) é necessária uma vez que
se a atualização por algum motivo não parou anteriormente, a distância entre os
pontos de cada iteração pode ser praticamente nula, o que leva para o anulamento
da região de confiança para \(d\), forçando que \(d = \overrightarrow 0\), o que
de novo leva para uma nova iteração sem movimento e iterando no mesmo ponto até
o limite de iterações.