% REVISÃO DE LITERATURA--------------------------------------------------------

\chapter{\larger Revisão Literária}
\label{chap:fundamentacaoTeorica}

É uma boa prática iniciar cada novo capítulo com um breve texto introdutório (tipicamente, dois ou três parágrafos) que deve deixar claro o quê será discutido no capítulo, bem como a organização do capítulo.
Também servirá ao propósito de "amarrar"{} o conteúdo deste capítulo com o conteúdo do capítulo imediatamente anterior.

\section{A programação sob o viés matemático}
Esboço de um resumo dos dois primeiros capítulos do livro de otimização, o que vai justificar muita coisa na verdade temos que ver depois v
\subsection{Otimizando à uma variável}
\subsection{Otimizando à mais de uma variável}

\section{As primeiras três décadas da programação}

Na década de 1930 o mundo encontrava-se em um estado onde problemas de gerenciamento
se tornaram muito complexos, e tão pouco estruturados, que modelos matemáticos e o poder
computacional existente na época foram postos à prova. A partir daí o campo de
pesquisa responsável por esses assuntos começou a tomar forma e ganhar mais importância.
Dentre os vários nomes dados a essa área de pesquisa, podemos encontrar pesquisa operacional,
análise de sistemas, análise de custo-benefício e análise de custo-efetividade. Durante essa
década, onde inciava-se a segunda guerra mundial, problemas como a alocação de recursos
limitados entre atividades simultâneas sob certas restrições eram comuns.
TODO: prólogo de preparação das subseções


\subsection{Kantorovich}

Em 1939, problemas de otimização de produções com base nas reservas da indústria (materiais,
trabalhadores e equipamentos) se tornaram essenciais no sustento do estado Soviético.
Segundo Kantorovich \cite{kantorovich1939}, existem duas formas de melhorar a eficiência
de uma fábrica. A primeira é melhorando a tecnologia empregada nos meios de produção, e a
outra é melhorando a organização do planejamento e da produção.

Antes disso, Kantorovich havia percebido que uma grande quantidade de problemas, de diversas
áreas, levavam à formulação de problemas de extremos. Mas ele também notou que esses problemas,
embora similares à problemas da análise matemática, quando se tentava resolver
usando os mesmos processos matemáticos, formalmente definidos, se tornavam inúteis, pois era
deveras custoso no aspecto computacional. Dentre um dos exemplos está a resolução de milhares
ou milhões de sistemas de equações.

Ele foi capaz de reduzir vários dos problemas que encontrou em três tipos.
Partindo do problema de alocação de tempo de uso de máquinas para a produção de partes que compõem
um dado produto, foi formulado um problema genérico considerando um número
\(n\) de máquinas, e a produção de \(m\) partes diferentes para confecção do produto. Usando
a \(i\)-ésima máquina para produzir apenas a \(k\)-ésima parte durante um dia inteiro de trabalho,
seria produzido \(\alpha_{i, k}\) partes. O problema consiste em dividir o tempo de trabalhado de cada
máquina de forma que possa ser construído o máximo possível de produtos.


Considerando \(h_{i, k}\) como a fração do dia de trabalho que a máquina \(i\) passou produzindo
a parte \(k\) sendo a variável que desejamos otimizar. Por motivos óbvios, ela deve ser positiva.
E também, por não podermos considerar que uma máquina esteja parada durante o dia de trabalho, temos
duas restrições:

\begin{enumerate}
  \item \(h_{i, k} \geq 0\), \(i =1, ..., m \);
  \item \(\sum_{k=1}^m h_{i, k} = 1\), \(i =1, ..., m \);
  \label{restricoes_a}
\end{enumerate}

Observando que a restrição \(2.\) significa que a soma das frações \(h_{i, k}\) deve resultar
em 1, visto que a unidade de tempo é o dia. Portanto, temos que a quantidade da \(k\)-ésima parte
produzida no dia é:

\begin{equation}
  z_k = \sum_{i=1}^n \alpha_{i, k} h_{i, k}.
  \label{zk}
\end{equation}


\par Para que se obtenha o máximo de produtos completos, é necessário que se tenha a mesma quantidade de
cada tipo de parte, do contrário foi perdido tempo produzindo partes extras. Com isso temos o
\textit{Problema A}, cuja configuração é dada pelas restrições a seguir:

\begin{center}
  Encontrar \(h_{i, k}\) tal que as partes \(z_k\) sejam todas iguais, isto é \(z_{k_1} = z_{k_2} \)
  para \(k_1, k_2 \in {1, ..., m}\). Onde \(z_k\) é dado pela expressão \ref{zk} e adicionalmente
  \(h_{i, k}\) satisfazendo as restrições descritas em \ref{restricoes_a}.
\end{center}

Problemas como o uso racional de combustível e distribuição ótima de terras aráveis
também foram usados como exemplos de problemas que podem ser reduzidos a mesma forma
que o Problema A.

Caso tenhamos mais condições limitantes, temos um outro modelo esperado por Kantorovich.
Considerando o mesmo problema de produção anteriormente definido, e agora com limite
máximo de energia gasta \(C\) para todas as máquinas e, \(c_{i, k}\) sendo o consumo de energia
da máquina \(i\) produzindo a parte \(k\) por um dia inteiro de trabalho, temos que o
Problema B é o Problema A com mais uma restrição:

\begin{align}
  \sum_{i=1}^n \sum_{k=1}^m c_{i, k} h_{i, k} \leq C
\end{align}

Podem existir vários significados sobre as quantidades em torno de \(C\), como por exemplo
a quantidade de pessoas disponíveis para trabalho, consumo de água ou qualquer recurso
necessário para a produção. Da mesma forma podemos ter diversas restrições como esta,
o que ainda é classificado como um problema tipo Problema B.

Ainda há um terceiro modelo de problema, quando uma mesma máquina pode produzir ao mesmo tempo
mais de uma parte, e que ela pode ser configurada para operar de diferentes métodos, com cada
método resultando em números diferentes de partes. Considerando \( \gamma_{i, k, l} \) o número de
\(k-esimas\) partes, produzidas na \(i\)-ésima máquina, utilizando a \(l\)-ésima configuração,
é preciso encontrar a fração de horas do dia trabalhado \(h_{i, l}\) que uma máquina \(i\).
O números de \(k\)-ésimas partes é dado por \( z_k = \Sigma_{i, l} \gamma_{i, k, l} h_{i, l}\). São aplicados as
restrições da mesma forma que para o Problema A e o Problema B. Com isso temos o Problema C:

\begin{align}
  max(z_k); \\
  z_1 = z_2 = ... = z_m; \\
  z_k = \sum_{i, l} \gamma_{i, k, l} h_{i, l} \\
  h_{i, l} \geq 0; \\
  \sum_l h_{i, l} = 1; \\
\end{align}


Para a resolução de tais problemas, Kantorovich desenvolveu um método simples. A partir do
Problema A, é certo que existem um conjunto de multiplicadores \( \lambda_1, \lambda_2, ..., \lambda_m \) com
cada um correspondendo a uma parte produzida, que são capazes de levar à solução do problema.
A partir deles podemos observar cada máquina \(i\) e escolher os multiplicadores que geram os
máximos entre \( \lambda_1\alpha_{i, 1}, \lambda_2\alpha_{i, 2}, ..., \lambda_m\alpha_{i, m} \). Em seguida, os multiplicadores \( \lambda_k \)
observados nas máquinas \(i\) que resultaram em máximos são postos como \(h_{i, k}\). Feito isso,
fica muito mais fácil determinar os outros valores para \( h_{i, k} \).

TODO: falar mais sobre esse método? ainda não entendi direito


\subsection{Dantzig}

\section{Programação não linear - O surgimento? Mas já tinha antes, só não com força}

Ao mesmo tempo que se refinavam as técnicas de otimização de problemas lineares, o que ocorreu
alguns anos após os estudos iniciais de Dantzig e Kantorovich, mais esforços foram dirigidos à
outras áreas da otimização. Problemas de otimização não linear já vinham sendo trabalhados
desde bem antes, principalmente problemas de otimização não restrita. O método de Newton, descrito
em \textit{De analysi per aequationes numero terminorum infinitas}, é um método capaz de encontrar
raízes de funções diferenciáveis, tomou a liderança dentre os métodos de otimizações usados para
tais problemas, e encontrou um "rival" cerca de dois seculos depois, o método do gradiente
descendente, proposto por Cauchy \cite{lemarechal2012cauchy} em 1847. Nesse contexto, após
a fixação de métodos para resolução de problemas lineares, foi iniciada uma busca por métodos para
problemas não lineares usando essas novas ferramentas. Alguns desses métodos servem como fundamentos
para o SCP, outros explicam as circunstancias históricas do SCP.

\subsection{J. E. Kelley}

Em 1960, a otimização restrita de problemas de mínimo encontrava pouco sucesso na época. No
entanto um pequeno subconjunto desses problemas estava gerando técnicas muito uteis, e um desses
subconjuntos era a minimização de uma função convexa em um conjunto fechado e convexo, que foi
alvo de muito interesse na época. O motivo de tanto interesse era que, como a função é convexa,
todo ponto de mínimo local é mínimo global.

Assim sendo, J.E. Kelley \cite{kelley1960cutting}, posto como responsável pela disseminação dos
métodos de plano cortante desenvolve um método de otimização capaz de resolver problemas convexos
de a partir de suposições mais genéricas. O método proposto por Kelley é iterativo e consiste na
resolução de problemas lineares.

Esse método faz parte do crescimento do estudo do problema de ajuste de curvas de Chebyshev
\cite{kelley1959computational}, \cite{kelley1958application}. O problema do ajuste de curvas pode
ser resolvido minimizando a seguinte situação:

\begin{align}
  \underset{\alpha}{\mathrm{min}}\ \sigma(\alpha, x) = f(x) - \Sigma_j \alpha_j g_j(x)
\end{align}

% TODO: explicar essa equação

Outros pesquisadores, como Cheney/Goldstein \cite{cheney1959newton} e Phillip Wolfe \cite{wolfe1960rand},
das condições de Wolfe, chegaram independentemente a este método, todos inspirados pelo trabalho de Remez.

Remez \cite{remez1934procede} desenvolveu um método de ajuste de curvas, que constrói o polinômio
com melhor aproximação para certas funções sob algumas condições. A forma de resolução é dada
pela resolução de problemas lineares, analise de erro, e atualização do estado atual baseado
em condições sobre o erro. Esses passos se tornaram base para toda a família de otimizadores
pelo fato de gerar informação sobre direções do ótimo baseado numa analise inteligente do
comportamento da função.

\subsection{R. E. Griffith, R. A. Stewart}

Em 1961, R. E. Griffith e R. A. Stewart \cite{griffith1961nonlinear} foram responsáveis por uma publicação
de caráter prático. Desenvolveram uma técnica de otimização não linear para sistemas de processamento
continuo, um método responsável por otimizar um problema da companhia de petróleo Shell. Trabalhando
de forma similar ao proposto por J.E. Kelley, usando resoluções de subproblemas lineares, o processo
consiste e resolver os subproblemas lineares de forma que a solução deles venham a convergir para
a solução do problema principal.

O método aceita problemas formulados respeitando um certo conjunto de regras, mas na pratica nem
sempre é necessário que sigam todas as funções visto que a solução dada é boa o suficiente. É dito
que as melhores formulações para o problema necessitam de que as variáveis sejam separadas em lineares
e não lineares; que as restrições envolvam tanto variáveis lineares e não lineares; que todas as funções
sejam continuas e preferencialmente bem comportadas. Um dos pontos positivos é que a escolha do ponto
inicial para dar inicio ao algoritmo iterativo não afeta tanto o resultado final.

A forma como a convergência das soluções dos subproblemas lineares resolvidos é mostrada é
bastante ingenua, o método sendo iterativo precisa de um ponto inicial, e tal ponto inicial é o
ponto dado como solução do subproblema linear resolvido anteriormente, e como a região onde esse
subproblema é resolvido respeita as restrições do problema principal, então a sequencia de pontos
gerados converge para a solução ótima dentro de polígonos gerados pela linearização das
restrições, e a solução é uma aresta desse polígono.

\subsection{S. P. Han}
Em 1977 S. P. Han \cite{han1977globally} publicou um artigo no qual demonstrava um método que mudaria
os rumos do estudo de otimização na época. Os métodos newtonianos que vinham sendo desenvolvidos
eram de convergência a um ótimo local apenas, até que S.P. Han mostrou um formato que a convergência
se tornaria a um ótimo global. Esse formato tem como objetivo resolver uma forma quadrática do
problema original, encontrando uma direção de descida, e, ao invés de analisar a otimização da
função original, analisa-se um função de mérito que leva em consideração propriedades locais da
função, tornando o método capaz de convergir para um ótimo global, e mais, de forma superlinear.

Após a publicação desse artigo, uma grande quantidade de publicações foram feitas usando como
base as técnicas de otimização sequencial quadrática (SQP), e até hoje, uma grande parte dos
otimizadores usam de alguma forma tais técnicas.

\subsection{Richard H. Byrd, Nicholas I.M. Gould, Jorge Nocedal, Richard A. Waltz}
Mesmo o SQP sendo um dos métodos mais bem sucedidos para otimização em larga escala, como com
problemas com centenas de variáveis e restrições, existe um problema, justamente neste caso, o
método se torna computacionalmente ineficiente. Por isso, Bryd, Gould, Nocedal e
Waltz \cite{byrd2003algorithm}, desenvolveram um novo método onde o problema quadrático não precisa
ser resolvido completamente, em que a resolução de um problema linear limite o que precisa ser
avaliado pelo problema quadrático, reduzindo assim o custo computacional da etapa mais pesada do
método, a resolução de problemas quadráticos. O que os métodos anteriores vinham fazendo era avaliar
as funções de restrição ao mesmo que tempo que se fazia a computação do atualização do ponto da
sequencia naquele momento. A mudança que este método fez foi separar cada etapa da resolução e
buscar uma melhor forma de resolver cada passo. O primeiro passo, feito por uma resolução de um
problema linear reduz o conjunto de restrições e define uma direção de minimização. Feito isso é
computado um modelo quadrático minimizando uma função de mérito que faz uso da direção dada pela
resolução do problema linear. Por fim, o problema quadrático é resolvido usando o conjunto de
restrições determinada na primeira etapa.


\section{Os pilares arquitetônicos do SCP}
\section{O modernismo sob o ponto de vista da programação (Uma ideia do o quê que a gente tem de moderno)}
