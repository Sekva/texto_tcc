% RESULTADOS-------------------------------------------------------------------

\chapter{\larger Análise e discussão dos resultados}

Cada capítulo deve conter uma pequena introdução (tipicamente, um ou dois parágrafos) que deve deixar claro o objetivo e o que será discutido no capítulo, bem como a organização do capítulo.



\section{O modernismo sob o ponto de vista da programação (Uma ideia do o quê que a gente tem de moderno)}
TODO: --
\begin{enumerate}
\item A partir de quando podemos dizer que os métodos de otimização passaram a ser modernos?
\item No contexto de programação, o que significa mordernismo?
\item Diante da analise historica apresentada no capitulo 2, sobre que contexto apresentaria o SCP?
\end{enumerate}






\section{Introdução ao SCP}

\begin{enumerate}
\item O que o método scp?
\item A que ele se aplica?
\item Para um primeiro entendimento, o quanto ele é eficaz?
\end{enumerate}

\subsection{Aplicação SCP em MINLP}
Uma das aplicações do SCP foi como parte de um otimizador de outra classe de problemas, na
otimização não linear inteira mista (MINLP). Existem problemas que buscam o ótimo usando
variáveis tanto discretas como continuas, com relações entre elas que são não lineares e
que também contém funções não convexas. Tudo isso dificulta muito a computação da solução.
Otimizadores capazes de resolverem esses tipos de problemas são ditos como os mais flexíveis,
já que seu escopo é tão abrangente. Isso o torna o centro das atenções por pesquisadores de
todas as áreas.

Problemas MINLP, na sua forma mais abstrata se dão da seguinte forma:

\vspace{-15pt}
\begin{flalign}
  &\underset{x}{\mathrm{min}}\ (f(x))\\
  &x \in  \mathcal{F} \subseteq \mathbb{R}^n
\end{flalign}

TODO: fazer o recuo
onde o conjunto \( \mathcal{F} \) pode ser tanto não linear quanto discreto, e \( f: \mathbb{R}^n \mapsto \mathbb{R}\) .
As diferentes escolhas
de \(f\) e \(\mathcal{F}\) levam as diferentes subclasses de MINLP, das quais temos algumas
denominações mais comuns, como:

\begin{itemize}
\item Otimização Convexa, onde \( \mathcal{F} \) é convexo, e o ótimo é inteiro;
\item Otimização Disjuntiva, onde algumas variáveis são continuas e outras são booleanas;
\item Otimização Não Linear, onde se usam métodos SQP;
\item Otimização com grafos de expressões;
\item Otimização por convexização e linearização.
\end{itemize}

Podemos encontrar que as formas mais gerais de MINLP são incomputáveis, visto que pode não existir
informações ou ferramentas para serem usadas na resolução do problema. Para problemas MINLP convexos,
e com restrições de desigualdades, a resolução de subproblemas lineares oferecem limites inferiores para
o ótimo, o que para o SCP, saber desses limites ajuda em questões de eficiência. Segundo os autores do
SCP, ao ser usado para problemas MINLP, foram encontradas soluções melhores em apenas um minuto do que
usando outros métodos que encontraram boas soluções em 12 horas.

A resolução de problemas MINLP convexos é importante no contexto global de problemas MINLP, já
que a maioria dos problemas são resolvidos por uma sequencia de problemas que foram convertidos em
problemas convexos. Experiências numéricas mostram que o SCP pode ser aplicado para problemas MINLP
não convexos como solucionador de subproblemas. A convergência ao ponto estacionário, quando aplicado
a estes tipos de problemas, aparenta ser rápida o suficiente mesmo que o estado do algoritmo esteja
longe do ponto estacionário.



\section{Análise Qualitativa do algoritmo - SCP}

TODO: copiar o pseudo codigo do scp
TODO: analisar minuciosamente cada etapada do algoritmo
TODO: ENTREGAR 07/06/2021 AS 16 O CAPTULO 4 TODO E INTEIRO

\subsection{O Algoritmo}
O algoritmo é iterativo, assim como no clássico método de Newton se é gerada
uma sequencia de pontos que se é esperada que convirja para um ponto ótimo.
As iterações são resoluções de problemas não lineares (iterações NLP), cada uma
dessas iterações consiste na resolução de um conjunto de subiterações de
resolução de problemas lineares. A cada problema linear resolvido se é obtida
uma direção de descida, e com essa direção, se é feita uma busca em linha,
buscando um tamanho ótimo para o passo. Após essas iterações de problemas
lineares, é esperado que um novo ponto capaz de reduzir suficientemente uma
função de mérito, o que garante a convergência.


\subsection{Problema resolvido pelo SCP}
O SCP resolve problemas em um formato razoavelmente amplo. Sendo:
TODO: definir o quanto é razoavel

\vspace{-15pt}
\begin{flalign}
  & min(f(x)) \\
  & g_j(x) \leq 0 \\
  & h_r(x) = 0 \\
  & j = 1, ..., m_i \\
  & r = 1, ..., m_e \\
  & x \in \mathbb{R}^n
\end{flalign}

Onde todas as funções devem ser continuamente diferenciáveis em \(\mathbb{R}^n\) assim como
devem ser funções \( \mathbb{R}^n \mapsto \mathbb{R} \). Nenhuma função é considerada convexa e isto
pode ser visto como um avanço à forma anterior do método. A única coisa assumida é que alguma das
funções \( g_j(x) \) é linear.

Outra coisa que é assumida, é que as qualificações de restrições de Mangasarian-Fromovitz estendidas (EMFCQ)
\cite{di1994exact} são válidas para qualquer \( x \in \mathbb{R}^n \) de forma que, \( \nabla h_r(x) \), para
\(r = 1, ..., m_e\), sejam linearmente independentes e que:

\vspace{-15pt}
\begin{flalign}
  & \exists z \in \mathbb{R}^n \\
  & \nabla g_j(x)^T z < 0, j \in (J_+(x) \cup J_0(x)) \\
  & \nabla h_r(x)^T z = 0, r = 1, ..., m_e
\end{flalign}
onde \( J_+(x) := \{j | g_j(x) > 0\} \) é o conjunto de restrições violadas e
\( J_0(x) := \{j | g_j(x) = 0\} \) é o conjunto de restrições que estão na fronteira
da região limitada por elas.

Estas qualificações de restrições garantem que, para qualquer ponto não viável, exista uma
direção de descida em direção a uma região viável. Existe também uma relação entre as EMFCQ
e funções de penalidade exatas.


\subsubsection{Subiterações lineares}

TODO: colocar indices de iterações embaixo

Cada subiteração linear é a resolução de um problema linear fixado no ponto
atual \( x_i \), \(i\) sendo a iteração linear atual. O problema linear que
será resolvido é dado por:

Minimizar:
\begin{equation}
  \nabla f(x^i)^T d + C(\sum_{j=1}^{m_i} t_j^g + \sum_{r=1}^{m_e} t_r^{h^+} + \sum_{r=1}^{m_e} t_r^{h^-})
  \label{funcao_objetivo_linear}
\end{equation}

Restrito à:

\begin{enumerate}[label=(\alph*), leftmargin=5em]
\item \( g_j(x^i) + \nabla g_j(x^i)^T d \leq t_j^g, j=1, ..., m_i \) \label{restricao_1}
\item \( h_r(x^i) + \nabla h_r(x^i)^T d = t_r^{h^+} - t_r^{h^-}, r=1, ..., m_e \) \label{restricao_2}
\item \( (d^r)^T (H^i d) = 0, r=1, ..., i-1, i > 1 \) \label{restricao_3}
\item \( d^L \leq d \leq d^U \) \label{restricao_4}
\item \( 0 \leq t_j^g \leq max(g_j(x^i), 0), j=1, ..., m_i \) \label{restricao_5}
\item \( 0 \leq  t_r^{h^+} \leq |h_r(x^i)|, r = 1, ..., m_e \) \label{restricao_6}
\item \( 0 \leq  t_r^{h^-} \leq |h_r(x^i)|, r = 1, ..., m_e \) \label{restricao_7}
\end{enumerate}

A solução sendo \( d, t^g, t^{h^+}, t^{h^-} \).

As restrições \ref{restricao_1} e \ref{restricao_2} são versões lineares das restrições originais,
onde a busca por um \(d_i \) que resolva o problema garante que na próxima iteração, o ponto já não
viole restrições. As variáveis \(t^g\), \(t^{h^+}\), \(t^{h^-}\), são relaxamentos das restrições,
o que facilita encontrar um \(d^i\) dentro de \ref{restricao_4}.

A restrição \ref{restricao_3} faz com que a direção de descida \( d^i \) seja H-ortogonal à todas
as outras direções encontradas anteriormente por outras subiterações lineares desta
interação não linear. \( H^i \) é a matriz hessiana da função Lagrangiana do problema
original, também avaliada no ponto \( x^i \).

Não é necessária mais que uma iteração de resolução de problema linear para
que o algoritmo convirja, mas ainda assim pode ser feito para melhorar a velocidade
de convergência. Como é necessário que a cada iteração linear a solução
\( d^i \) sejam gradientes conjugados, o algoritmo busca uma direção de descida que
"aponte" mais certeiramente para o ótimo. Por isso os autores dizem que o método
também lembra o método do gradiente conjugado para problemas irrestritos.

A restrições \ref{restricao_4} nada mais são que um paralelepípedo que limita o espaço de busca de uma
direção descida. Onde \( d^L < \overrightarrow 0 \) e \( d^U > \overrightarrow 0 \).

As linearizações vistas em \ref{restricao_1} e \ref{restricao_2} e o requerimento de ortogonalidade entre
as direções de descida encontradas em respeito à Lagrangiana da função em \ref{restricao_4} são (hiper)planos
que cortam o espaço.

As restrições \ref{restricao_1} são linearizações que geram uma região no espaço que aproxima a região viável
das restrições \( g_j(x), j=1, ..., m_i \). Da mesma forma \ref{restricao_2} são (hiper)planos que aproximam
também a região limitada de \( h_r(x), r=1, ..., m_e \). Por fim, \ref{restricao_3} também gera (hiper)planos
e faz com que as direções de descida \( d^i \) estejam nesses (hiper)planos.

\section{Análise Quantitativa do algoritmo - SCP}
\section{As vantagens e desvantagens computacionais do SCP}
\section{Um modelo algorítmico para o SCP}
